{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delivery Duration Prediction\n",
    "###### This is a machine learning project made with Doordash data. It is possible to find it at the following link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importng Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: # To visualize the plots in jupyter notebooks\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd #Dataframe creation and manipulation (any operation on data)\n",
    "import numpy as np #Numerical mathematical operation\n",
    "import matplotlib # Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Visualization\n",
    "%matplotlib inline # To visualize the plots in jupyter notebooks\n",
    "\n",
    "sns.set_style(\"darkgrid\") \n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)\n",
    "matplotlib.rcParams['figure.facecolor'] = '#00000000'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv('historical_data.csv', parse_dates=['created_at', 'actual_delivery_time']) #to parse the column in date time format \n",
    "# parse_dates function of pandas to define the range from creayed_At to actual_delivery_time\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.describe()\n",
    "# mean, median- when mean is near to the 75% quartile data then mean is used\n",
    "# subtotal mean lies between 505 and 75% quartile i.e data is right skewed thus we will choose mean for null value replacement, most of our data lies after 50%\n",
    "# mode- If 25, 50 and 75% quartile data are nearly same then that means the most frequency of the data lies in one freq called mode\n",
    "# max value can be a outlier\n",
    "# If the data has less variance then we use mode for null value treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting null values using heatmap\n",
    "plt.figure(figsize=(30,5))\n",
    "sns.heatmap(dd.isna())\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Since there are only seven missing values for one of the feature that will compose the target, it is possible to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_to_remove = dd[dd.actual_delivery_time.isna()].index\n",
    "dd.drop(index=obs_to_remove, inplace= True) # inplace is used to perform operation on existing dataframe without assigning to the variable\n",
    "# To assign the changes to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again counting the number of values after dropping\n",
    "dd.actual_delivery_time.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['Target'] = (dd.actual_delivery_time - dd.created_at).dt.seconds\n",
    "dd.Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Columns with missing values:\n",
    "\n",
    "* market_id\n",
    "* store_primary_category\n",
    "* order protocol\n",
    "* total_onshift_dashers\n",
    "* total_busy_dashers\n",
    "* total_outstanding_orders\n",
    "* estimated_store_to_consumer_driving_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.market_id.value_counts() # Counts of particular market id, count the values in each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ID 2.0 seems to be the most popular one, since there are not many information regarding the geographical position a possible solution is to replace the null values with 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.market_id.fillna(2.0, inplace=True)\n",
    "print('Null values: {}'.format(dd.market_id.isna().sum()))\n",
    "# print(f'Null values: {dd.market_id.isna().sum()}')\n",
    "dd.market_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store primary category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.store_primary_category.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''unique_id = dd.store_id.unique()\n",
    "unique_id\n",
    "unique_id = dd.store_id.unique().tolist()# Convert array to list'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = pd.DataFrame() # create blank dataframe\n",
    "dd[dd.store_id == 1845]['store_primary_category'].mode()[0]\n",
    "# For store_id 1845 we found the mode of column store_primary_category to replce the null value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The missing values in the store primary category can be replaced by using the store_id feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = dd.store_id.unique().tolist()\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for x in unique_id:\n",
    "    try:\n",
    "        pop = dd[dd.store_id == x]['store_primary_category'].mode()[0]\n",
    "        store_df = dd[dd.store_id == x]['store_primary_category'].fillna(pop)\n",
    "        new_df = pd.concat([new_df, store_df])\n",
    "    except:\n",
    "        df_to_concat = dd[dd.store_id == x]['store_primary_category']\n",
    "        new_df = pd.concat([new_df, df_to_concat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[0].isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['store_primary_category'] = new_df[0] # Store the new column 0 to store_primary_category\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The number of missing values dropped from 4760 to 867, in order to replace the remaining ones it could be adopted the same process but using the location feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_sub(id):\n",
    "    popular = dd[dd.market_id == id]['store_primary_category'].mode()[0] # Here we are finding the mode for id\n",
    "    market_dict[id] = popular # Appending the mode to the market_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_id_unique = dd.market_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[market_sub(id) for id in market_id_unique]\n",
    "market_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the null values\n",
    "null_primary_df = dd[dd.store_primary_category.isna()]\n",
    "\n",
    "# Replacing them\n",
    "null_primary_df['store_primary_category'] = null_primary_df.market_id.map(market_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null indexes\n",
    "null_indexes = null_primary_df.index \n",
    "dd.drop(index=null_indexes, inplace=True)\n",
    "\n",
    "# Concatenating \n",
    "dd = pd.concat([dd, null_primary_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.store_primary_category.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### All the null values have been replaced.\n",
    "\n",
    "###### Let's see what are the remaining ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.order_protocol.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty df\n",
    "order_p_df = pd.DataFrame()\n",
    "\n",
    "# Replacing values\n",
    "for x in unique_id:\n",
    "    try:\n",
    "        most_popular = dd[dd.store_id == x]['order_protocol'].mode()[0]\n",
    "        rep_df = dd[dd.store_id == x]['order_protocol'].fillna(most_popular)\n",
    "        order_p_df = pd.concat([order_p_df, rep_df])\n",
    "    except:\n",
    "        null_protocol = dd[dd.store_id == x]['order_protocol']\n",
    "        order_p_df = pd.concat([order_p_df, null_protocol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_p_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['order_protocol'] = order_p_df[0]\n",
    "dd.order_protocol.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Only three null values remains, they could be substituted with the most common order protocol for that food."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the remaining stores with null order protocol\n",
    "null_stores = dd[dd.order_protocol.isna()]['store_id'].unique().tolist()\n",
    "\n",
    "# Creating an empty df\n",
    "remaining_null = pd.DataFrame()\n",
    "\n",
    "# Substituting\n",
    "\n",
    "for x in null_stores:\n",
    "    food_category = dd[dd.store_id==x]['store_primary_category'].mode()[0]\n",
    "    order_protocol_for_food = dd[dd.store_primary_category == food_category]['order_protocol'].mode()[0]\n",
    "    sub_df = dd[dd.store_id == x]['order_protocol'].fillna(order_protocol_for_food)\n",
    "    remaining_null = pd.concat([remaining_null, sub_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[dd.order_protocol.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null order protocol \n",
    "null_order_protocol = dd[dd.order_protocol.isna()]\n",
    "\n",
    "# Substituting\n",
    "null_order_protocol['order_protocol'] = remaining_null[0]\n",
    "\n",
    "# Null values indexes to drop \n",
    "drop_index = dd[dd.order_protocol.isna()].index \n",
    "\n",
    "# Dropping\n",
    "dd.drop(index=drop_index, inplace=True)\n",
    "\n",
    "# Adding the new values\n",
    "dd = pd.concat([dd, null_order_protocol])\n",
    "\n",
    "# Checking the result\n",
    "dd.order_protocol.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total busy dashers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Replacing with the average value of the store, else, with the average vvalue of the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dash_and_orders(column):\n",
    "    df_to_append = pd.DataFrame()\n",
    "    try:\n",
    "        for x in unique_id:\n",
    "            avg = dd[dd.store_id == x][column].mean()\n",
    "            df_remove_null = dd[dd.store_id == x][column].fillna(avg)\n",
    "            df_to_append = pd.concat([df_to_append, df_remove_null])\n",
    "    except:\n",
    "        df_with_null = dd[dd.store_id==x][column]\n",
    "        df_to_append = pd.concat([df_to_append, df_with_null])\n",
    "    return df_to_append "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total busy dashers\n",
    "busy_dashers = dash_and_orders('total_busy_dashers')\n",
    "busy_dashers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['total_busy_dashers'] = busy_dashers[0]\n",
    "dd.total_busy_dashers.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now, it is possible to use the average of the market id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mkt_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_mkt(id):\n",
    "    avg = round(dd[dd.market_id == id]['total_busy_dashers'].mean())\n",
    "    avg_mkt_dict[id] = avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[avg_mkt(id) for id in market_id_unique]\n",
    "avg_mkt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with null values\n",
    "df_to_add = dd[dd.total_busy_dashers.isna()]\n",
    "\n",
    "# Finding indexes to drop\n",
    "indexes_to_remove = dd[dd.total_busy_dashers.isna()].index \n",
    "\n",
    "# Substituting values\n",
    "df_to_add['total_busy_dashers'] = df_to_add.market_id.map(avg_mkt_dict)\n",
    "\n",
    "# Removing values\n",
    "dd.drop(index=indexes_to_remove, inplace=True) \n",
    "\n",
    "# Adding the new ones\n",
    "dd = pd.concat([dd, df_to_add])\n",
    "\n",
    "# Checking the result\n",
    "dd.total_busy_dashers.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total onshift dashers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onshift_dashers = dash_and_orders('total_onshift_dashers')\n",
    "onshift_dashers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['total_onshift_dashers'] = onshift_dashers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onshift_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_mkt(id, column):\n",
    "    avg = round(dd[dd.market_id == id][column].mean())\n",
    "    onshift_dict[id] = avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[avg_mkt(id, 'total_onshift_dashers') for id in market_id_unique]\n",
    "onshift_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with null values\n",
    "df_to_add = dd[dd.total_onshift_dashers.isna()]\n",
    "\n",
    "# Finding indexes to drop\n",
    "indexes_to_remove = dd[dd.total_onshift_dashers.isna()].index \n",
    "\n",
    "# Substituting values\n",
    "df_to_add['total_onshift_dashers'] = df_to_add.market_id.map(onshift_dict)\n",
    "\n",
    "# Removing values\n",
    "dd.drop(index=indexes_to_remove, inplace=True) \n",
    "\n",
    "# Adding the new ones\n",
    "dd = pd.concat([dd, df_to_add])\n",
    "\n",
    "# Checking the result\n",
    "dd.total_onshift_dashers.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total outstanding orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outstanding_orders = dash_and_orders('total_outstanding_orders')\n",
    "outstanding_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['total_outstanding_orders'] = outstanding_orders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outstanding_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_mkt(id, column):\n",
    "    avg = round(dd[dd.market_id == id][column].mean())\n",
    "    outstanding_dict[id] = avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[avg_mkt(id, 'total_outstanding_orders') for id in market_id_unique]\n",
    "outstanding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with null values\n",
    "df_to_add = dd[dd.total_outstanding_orders.isna()]\n",
    "\n",
    "# Finding indexes to drop\n",
    "indexes_to_remove = dd[dd.total_outstanding_orders.isna()].index \n",
    "\n",
    "# Substituting values\n",
    "df_to_add['total_outstanding_orders'] = df_to_add.market_id.map(outstanding_dict)\n",
    "\n",
    "# Removing values\n",
    "dd.drop(index=indexes_to_remove, inplace=True) \n",
    "\n",
    "# Adding the new ones\n",
    "dd = pd.concat([dd, df_to_add])\n",
    "\n",
    "# Checking the result\n",
    "dd.total_outstanding_orders.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimated store to consumer driving duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = dash_and_orders('estimated_store_to_consumer_driving_duration')\n",
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['estimated_store_to_consumer_driving_duration'] = duration[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_mkt(id, column):\n",
    "    avg = round(dd[dd.market_id == id][column].mean())\n",
    "    duration_dict[id] = avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[avg_mkt(id, 'estimated_store_to_consumer_driving_duration') for id in market_id_unique]\n",
    "duration_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with null values\n",
    "df_to_add = dd[dd.estimated_store_to_consumer_driving_duration.isna()]\n",
    "\n",
    "# Finding indexes to drop\n",
    "indexes_to_remove = dd[dd.estimated_store_to_consumer_driving_duration.isna()].index \n",
    "\n",
    "# Substituting values\n",
    "df_to_add['estimated_store_to_consumer_driving_duration'] = df_to_add.market_id.map(duration_dict)\n",
    "\n",
    "# Removing values\n",
    "dd.drop(index=indexes_to_remove, inplace=True) \n",
    "\n",
    "# Adding the new ones\n",
    "dd = pd.concat([dd, df_to_add])\n",
    "\n",
    "# Checking the result\n",
    "dd.estimated_store_to_consumer_driving_duration.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### All the null values have been removed.\n",
    "\n",
    "###### Saving the new dataframe in a separate csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.to_csv('dd_no_null.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Inputs and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv('dd_no_null.csv', parse_dates=['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some columns can be dropped:\n",
    "\n",
    "* actual_delivery_time\n",
    "* store_id\n",
    "* store_primary_category\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['store_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['actual_delivery_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dd.drop(columns=['actual_delivery_time', 'store_id', 'Target'])\n",
    "y = dd.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering on X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New columns related to the creation's date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['year'] = X.created_at.dt.year \n",
    "X['month'] = X.created_at.dt.month \n",
    "X['day_of_week'] = X.created_at.dt.day\n",
    "X['hour'] = X.created_at.dt.hour \n",
    "X['minutes'] = X.created_at.dt.minute  \n",
    "X['day_of_year'] = X.created_at.dt.strftime('%Y-%U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now it is possible to drop the 'created_at' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns='created_at', inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ideas for feature engineering\n",
    "* price per item: subtotal/total_items\n",
    "- busy dashers over total dashers:        \n",
    "- total_busy_dashers/total_onshift_dashers\n",
    "- price range: max_item_price-min_item_price\n",
    "- number of dashers per outstanding order: (total_onshift_dashers)/total_outstanding_orders\n",
    "- price range / (total_items)^2 = (max_item_price - min_item_price) / np.square(total_items)\n",
    "- order + drive = estimated_order_place_duration + estimated_store_to_consumer_driving_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price per item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['price_per_item'] = X.subtotal/X.total_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of dashers per outstanding order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['dashers_order_ratio'] = X.total_onshift_dashers/(X.total_outstanding_orders+7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price range on items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['price_on_items'] = (X.max_item_price - X.min_item_price) / np.square(X.total_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order + drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['order_and_drive'] = X.estimated_order_place_duration + X.estimated_store_to_consumer_driving_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Categorical Columns to Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_cols = ['market_id', 'order_protocol', 'store_primary_category'] \n",
    "\n",
    "# encoder = OneHotEncoder()\n",
    "\n",
    "# X[encoded_cols] = encoder.fit_transform(X[categorical_cols]) \n",
    "\n",
    "# encoded_cols = encoder.get_feature_names(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
